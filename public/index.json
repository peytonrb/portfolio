
[{"content":"\rOverview\r#\rInterweaver\u0026rsquo;s gameplay was unique in that the player\u0026rsquo;s main character swapped between two different characters with unique mechanics per level: the Weaver, our main character, and a Familar that was unique to each Level. The Weaver has two core player mechanics that were the root of all gameplay and drove the majority of interactions, a Weave ability and a Possession ability.\nAs part of my role as an Engineer, I was responsible for developing the entire Weave ability and its components. This involved a game system that allowed the Player to manipulate objects in the game world by translating, rotating, and linking other objects together to solve physics-based puzzles and platforming challenges. I also was responsible for a variety of features relating to both the Weaver and the Familiars, which will also be discussed below.\nWeave Ability\r#\rThe Weaver\u0026rsquo;s Weave ability is a physics-based interaction mechanic that allows players to manipulate objects in game space using the mouse. Weaving has two main functions: moving objects around the space and combining multiple to solve puzzles or progress through platforming sections. Players were also allowed to rotate objects as needed when Weaving.\nSystem Architecture\r#\rWeaving was managed through a Weaveable Manager, a custom GameObject for all Weaveable objects, and a Controller script on the Weaver, separating logic to keep the system modular, easy to iterate on, and debug.\nI want to call out a few aspects of this system that presented notable technical and design challenges, and discuss how they were resolved.\nInput-To-World Space Mapping\nThe core challenge wasn\u0026rsquo;t just mapping the mouse to world space - it was ensuring that every interaction made sense in both the player\u0026rsquo;s and the Weaver\u0026rsquo;s perspectives in an isometric environment, and was consistent across input devices. I accomplished this using a combination of raycasting and precise physics handling to achieve smooth and intuitive movement, and also ensured interactions with the environment were visually and functionally consistent. This was made further complex because Interweaver supports seamless transitions between controller and keyboard inputs, which had different contextual behaviors. I implemented a system that allowed both input methods to switch seamlessly as players connected or disconnected controllers, ensuring the mechanics adapted properly and the gameplay experience remained uninterrupted.\nWeaveWithMouse() { // Find the main camera ... Ray ray = mainCamera.ScreenPointToRay(Input.mousePosition); // If this object is able to be Woven.. if (Physics.Raycast(ray, out RaycastHit hit, 100f, layersToHit)) { // Speed and position constraints rb.velocity = new Vector3(raycastHit.point.x - rb.position.x, transform.position.y - rb.position.y, raycastHit.point.z - rb.position.z); rb.constraints = RigidbodyConstraints.FreezeRotationX | RigidbodyConstraints.FreezeRotationY | RigidbodyConstraints.FreezeRotationZ; } RaycastHit hitData; if (Physics.Raycast(ray, out hitData, 1000)) { float rayDistance = Vector3.Distance(gameObject.transform.position, hitData.point); // If the rayDistance is longer than maxDistance, all Woven objects are automatically dropped ... else { // Cache the last valid world-space target position worldPosition = hitData.point; } } }\rWeaveWithController(Vector2 lookDir) { // Raycast starts from initial look direction of Main Character rather than mouse point float targetAngle = Mathf.Atan2(lookDir.x, lookDir.y) * Mathf.Rad2Deg + mainCamera.transform.eulerAngles.y; TargetObject.transform.rotation = Quaternion.Euler(0, targetAngle, 0); Vector3 rayDirection = TargetObject.transform.forward; // If look direction exists and object can be Woven rb.velocity = rayDirection * 6; // Freezes the Y position so that the combined objects won\u0026#39;t drag it down because of gravity and it freezes in all rotation so it won\u0026#39;t droop because of the gravity from the objects rb.constraints = RigidbodyConstraints.FreezeRotationX | RigidbodyConstraints.FreezeRotationY | RigidbodyConstraints.FreezeRotationZ; ... }\rThe snippets above demonstrate the differences in functionailty between the two input systems. Weaving from the mouse requires raycasting based on screen position, while the controller relies on calculating movement based on the character\u0026rsquo;s facing direction and applying it in world space. Both needed to interact with the same physics system, feel consistent to the player, and intuitively adapt as players swapped input devices.\nCombining Weaveables and Snapping Points\nAnother part of the system that presented a bit of a technical challenge was the ability for the Player to organically combine objects using the Weave ability. This feature allowed players to manipulate and rotate any cluster of Weaveable objects, so they could be used in puzzles and platforming challenges uniquely by each individual.\nThe challenge here was ensuring that objects connected both visually and geometrically, so clusters of objects could be moved into precise areas without awkward or inconsistent geometry. To address this, I created snapping points, which were points in space that could be manually or automatically applied to Weaveable objects. This made it easy for designers to work with as we continued to added puzzles and iterated on their complexity, even as we worked with organically shaped objects.\nSnapping() { ... // Nearest-neighbor resolution to ensure geometric correctness foreach (GameObject point in transformPoints) { foreach (GameObject otherPoint in weaveableScript.transformPoints) { float distance = Vector3.Distance(point.transform.position, otherPoint.transform.position); if (distance \u0026lt; nearestDistance) { nearestDistance = distance; myClosestPoint = point; otherClosestPoint = otherPoint; } } } // Determined Coroutine to trigger to move objects visually StartCoroutine(MoveToPoint(this, transform, WeaveableToReach)); }\rIEnumerator MoveToPoint(Weaveable source, Weaveable target, Transform transform) { ... float timeSinceStarted = 0f; while (true) { timeSinceStarted += Time.deltaTime; movingWeaveableRef.transform.position = Vector3.Lerp(movingWeaveableRef.transform.position, targetTransform.position, timeSinceStarted); if (Vector3.Distance(movingWeaveableRef.transform.position, targetTransform.position) \u0026lt; 1f) { movingWeaveableRef.rb.transform.position = targetTransform.position; if (!TryGetComponent\u0026lt;FixedJoint\u0026gt;(out FixedJoint fJ)) { // Systematically combines the objects and calls any additional visualizations or logic WeaveTogether(otherWeaveable.gameObject); } yield break; } yield return null; } }\rI tried to illustrate in the snippets above how combining Weaveables utilized nearest-neighbor resolution were resolved and applied at runtime to ensure the mechanic felt smooth and enjoyable to play. Rather than relying on mesh bounds, the system finds the closest authored or generated snapping point on both objects and snaps to the most geometrically appropriate connection, ensuring the object\u0026rsquo;s rotation and movement remained visually coherent within the game space. This helps the player predict the outcome of their actions while also enabling them to create unique solutions to the game\u0026rsquo;s challenges and approach gameplay in their own way.\nOnce valid snap point pairings were found, I used a set of Coroutines to smoothly interpolate objects into position before finalizing the connection visually and systematically. These different Coroutines prescribed different movement types that objects would take in space to attach to the Woven objects, rules that I ensured were deterministic and easily configuable. By separating this logic, it was easy to identify bugs both visually and systematically, and it also ensured the interactions felt natural and fun for the player.\n","externalUrl":null,"permalink":"/games/interweaver/player-mechanics/","section":"Games","summary":"","title":"Player Mechanics","type":"games"},{"content":"\rOverview\r#\rProject Burbank allowed the player to take the role of the Director in the campaign they played out during the game. Burbank uniquely allowed players to customize not just their Main Character and the narrative they were directing, but allowed them to edit a cast of characters\u0026rsquo; memories and backstories, change the genre of the story, and more systemic characteristics. I was responsible for the Pre- and Post-Game systems, ensured that they created an experience as engaging as the core gameplay, and they became integral parts of the gameplay loop as a result.\nThese systems were instrumental in establishing the core experience for the player and also ensured the game remained compelling for those who did not want to direct the entire experience. Both Pre- and Post-Game flows consisted of a series of stages containing a mix of UI and game camera that the player progressed through to set up and view the results of their campaign. These systems were built in C++ with Blueprint and required heavy use of MVVM, State Tree, TFutures and TPromises, and more.\nIn-Game Representation\r#\rPre-Game\r#\rThe Pre-Game flow was responsible for guiding the player through character and campaign customziations, and had Stages like Character Customization, Cast Customization, Campaign and Story Setup. It was essential that Pre-Game not only introduced players to all the system-defining customization options, but was also a creative outlet and fun in its own right.\nSome examples of Pre-Game Stages: Post-Game\r#\rThere are many unique and interesting ways to display game stats at the end of a campaign, which made designing this system even more fun. The Post-Game flow consisted of Studio Notes (our recap screen), Level Up, and the Next Scene Chooser. Because each playthrough was so interactive and unique to itself, it was important that we encapsulated the nuances of each playthrough in this flow.\nSome examples of Post-Game Stages: System Architecture\r#\rEach Stage of the Pre- and Post-Game flows were treated as individual entities, though they existed in the same world and were pre-loaded to avoid any additional buffer. A State Tree managed the entirety of the flow due to its modular nature, and each player-facing step was treated as a State. Each State followed the same general pattern: A custom C++ Manager Actor was spawned as it entered the State and gets assigned the associated UI\u0026rsquo;s View Model.\nThe Manager Actor and View Model manage the all of the gameplay logic and update the widget via bindings. The majority of the logic is kept in C++ to avoid checking out Assets in Perforce to maintain design and iteration speed.\nModular State Tree-Based Design\r#\rEach player-facing screen was treated as its own individual State, and each State was able to have unique children States that allowed us to further customize the internal functionality of each Stage. This was exceptionally important due to the asynchronous nature of the systems in a few Stages.\nPre-Game Post-Game In Stages where the player is not able to progress until internal asynchronous background logic is processed, I utilized \u0026lsquo;In Progress\u0026rsquo; child stages to stop progression until the Manager Actor broadcasts a delegate to flag that all asynchronous processes have fully finished and it is safe to transition. This modular design also makes it easy to swap out or add in screens without editing any \u0026lsquo;Back\u0026rsquo; or \u0026lsquo;Next\u0026rsquo; functionality by hand.\nMVVM Patterns\r#\rTo manage the UI widgets per stage and the game state itself, I utilized MVVM to connect player-inputted data to internal gameplay systems to dynamically generate new narrative campaigns and to collect and record campaign state data in these two systems.\nBelow is an example of the View Model on the Character Creator Stage: Since the View Model references the Manager Actor and vice versa, making changes or running logic for the UI from C++ becomes very simple. View Models are heavily used on all Stages during these flows, and handled a variety of tasks from updating UI, processing player-inputted data, and customizing each individual playthrough. The base Manager Actor and View Model classes set this functionality up by default for ease of creating new Stages in the future.\nTFutures and TPromises\r#\rAsynchronous coding was heavily used during these flows because Stages were dependent on the information gained from the Stage prior - however not all necessary data was instantly transmitted or readable. This created a lot of complexity as I was designing the system and was the forcing function behind a few systemic decisions that were made.\nIt was especially common to require multiple Actors or specs that were created asynchronously during this process. Below I\u0026rsquo;ve attached a pseudocode snippet of a pattern I employed heavily to manage this requirement:\nTSharedPtr\u0026lt;TPromise\u0026lt;TArray\u0026lt;ACharacterActor*\u0026gt;\u0026gt;\u0026gt; Promise = MakeShared\u0026lt;TPromise\u0026lt;TArray\u0026lt;ACharacterActor*\u0026gt;\u0026gt;\u0026gt;(); TWeakObjectPtr\u0026lt;ACharacterActor\u0026gt; WeakSelf(this); CreateCharacterAsync().Next([WeakSelf, Promise](const TArray\u0026lt;FNewCharacterSpec\u0026gt;\u0026amp; CharacterSpecs)) --\u0026gt; void { if (!WeakSelf.IsValid()) { Promise-\u0026gt;EmplaceValue(TArray\u0026lt;ACharacterActor*\u0026gt;()); return; } TArray\u0026lt;TFuture\u0026lt;ACharacterActor*\u0026gt;\u0026gt; Futures; for (FInstancedStruct Spec : CharacterSpecs) { ... Futures.Add(CreateCharacterFromSpec(*Spec)); } WhenAll(MoveTemp(Futures)).Next([Promise](const TArray\u0026lt;ACharacterActor*\u0026gt;\u0026amp; Results) { Promise-\u0026gt;EmplaceValue(Results); }); }); return Promise-\u0026gt;GetFuture();\rWhere:\nTFuture\u0026lt;ACharacterActor*\u0026gt; CreateCharacterFromSpec(const FInstancedStruct\u0026amp; CharacterSpec);\rThis snippet is one instance of a broader pattern that I used throughout the system. Rather than letting async work leak into gameplay-facing systems, I chained async steps together and wrapped the result in a single return value, keeping other dependent and higher-level systems clean and latent. This made the base code easier to iterate on, safer under lifecycle or loading changes, and much easier to debug as the system grew.\nReflection\r#\rThese systems are foundational to how players understand, build, and reviewed their campaigns, meaning it was essential they were as fun and reliable as the core game itself. Building them reinforced the importance of structure and pacing for these supporting experiences, and evolved my approach to designing these UI-driven systems:\nState-driven architecture allowed each screen to exist as an independent stage, making it easy to add, remove, or reoder Stages as needed without rewriting UI navigation logic, introducing dependencies, and more. Pairing Manager Actors with ViewModels created a clear separation of gameplay logic and UI presentation, keeping it easy to debug and author as well as giving space to designers and UI artists. Having each screen as a self-contained system with shared base behavior made it easy to iterate upon each individual Stage without impacting downstream screens. Encapsulating asynchronous code with TFutures and TPromises prevents timing complexities from leaking into gameplay-facing code, which minimizes the risk of race conditions and improves code maintainability - an important quality during rapid iteration. ","externalUrl":null,"permalink":"/games/burbank/pre-post-game/","section":"Games","summary":"","title":"Pre- and Post-Game Flow","type":"games"},{"content":"\rOverview\r#\rProject Burbank is a storytelling platform for players to immerse themselves in any narrative imaginable - it takes players\u0026rsquo; decisions and alters the campaign at hand to create surprising, one-of-a-kind gameplay experiences. I supported this characteristic by creating a procedural interior decoration system that intakes information about the player\u0026rsquo;s campaign at runtime and filling subsequent Levels with contextually appropriate assets, while also maintaining navigable space and realistic orientations.\nThis system was not only instrumental in establishing the mood and feel of a Level in game, but it was also visually and mechanically obvious if there were any logical flaws. It also had to handle external systems like lighting, BP Actors in the Level, player-placed objects, cameras, and more. It was imperative that I ensured this system was able to handle not only our technical limitations, but also uplifted the player\u0026rsquo;s experience.\nThis system was built in C++ with Blueprint, and utilized World Partition, PCG, and more.\nIn-Game Representation\r#\rEach Level is constructed at design-time with base 3D models as a greybox to outline the general shape and orientation of objects in space. Each Level in UE is constructed with a variety of Data Layers containing different Socket Actors to hold contextual data and act as a base in order to preserve character pathfinding and Actor Slot requirements. The Decoration system then reads the Socket Actors and the current campaign information and chooses assets to sort and where to place them accordingly.\nYou can see 2 base Layout examples from our Apartment Level below: After the Decoration system finishes placing assets on the determined base layout, the final in-game Level now contextually fits the unique story being played out and also furthers player immersion: Technical Spec\r#\rAs players progressed through their narrative campaigns, it was important that the Levels their characters spawned into visually complimented and matched the aesthetic of the story being played out. This procedural interior Level decoration system intakes data about the active Story, available Assets to populate the space, and more to produce an accurately dressed Level.\nBelow I\u0026rsquo;ve summarized the official Technical Design Document I wrote for this system to give an idea of how it worked.\nSummary\r#\rEach World Partition Level was built as a greyboxed space using templated Data Layers instead of fixed Assets. Designers constructed these Layers with Sockets, which were Assets embedded with metadata and natural language descriptions responsible for holding space for NavMesh and cinematic cameras. The Decoration system queried the Sockets in the space once it chose the most relevant Data Layer and populated them with Assets relevant to the runtime narrative. From there, designer and auto-placed Inclusion Volumes marked up where PCG spawns non-blocking decorative clutter, ensuring the space appeared engaging, immersive, and relevant to the story at the end.\nAll of this logic exists in a Plugin and includes an Editor Module for custom property, detail, and editor customizations to support designers as they construct each Level.\nCustom Classes\r#\rDecoDataLayer\nThe need for DecoDataLayers arose when I realized that a full procedually decorated interior space, also because the player is able to edit their space, would cause too much instability between character navigation, cinematic cameras, and more. DecoDataLayers are extended from the base DataLayerAsset class and are used to create all Data Layers that can be evaluated by the Decoration system.\nSome Properties include:\nA natural language FText description of the space and example use-cases Gameplay Tag container for relevant context tags DecoSocketComponent\nThe DecoSocketComponent was added in-editor to any Actor in a DecoDataLayer to greybox the space. This component dictates what Actors and what properties about them are swappable. These Components subscribe to policies that dictate what about the Actor they are on can be affected by the system. Example policies are:\nBlueprint: the entire BP Actor can be changed Static Mesh: the Mesh of the Actor can be changed Material: certain Materials can be changed Group: the Actor is changed exactly the same as another Socket These Components allowed us to build more modular Data Layers because we could nest Components with varying degrees of complexity for a more unique space each playthrough (ie. a Couch with a Material Component policy could be placed by the Deco system in an Component that allows BP subtitution). These Sockets are responsible for applying their own customizations once they are given narrative details by the DecoActor. It is also possible to override query behavior on Sockets for different Assets, so not all Sockets evaluate the Asset Libraries the same as well.\nSome Properties on the Socket Components include:\nThe policy to follow A natural language FText description of the expected object Gameplay Tag container for relevant context tags It is also important to note here that the Transforms and orientations of Assets is a major constraint here. To account for this, the Asset Libraries themselves reject Assets that would not fit in the expected Sockets to avoid hard-to-follow bugs down the line.\nDecoActor\nThe DecoActor is responsible for centralizing most of the logic pertaining to the Decoration System. A single DecoActor is spawned automatically the first time a Level is loaded into a Show for a particular interior Set at runtime. It is important to mention that a Level can get used for multiple different Set types (ie. the \u0026lsquo;Bedroom\u0026rsquo; level could get separate Decorations for \u0026lsquo;Character A\u0026rsquo;s Bedroom\u0026rsquo; and \u0026lsquo;Chararacter B\u0026rsquo;s Bedroom\u0026rsquo;). Therefore, one Level may have multiple different runtime occurences of DecoActors as the player progresses in their campaign.\nDecoActors handle the decoration of a Set when they get initialized and are also responsible for polishing existing Decorations as Sets are revisited. If the player revisits a Set with an existing DecoActor, a new one is not created and the existing one redecorates the room.\nSome other responsibilities include:\nMaintaining Sockets Querying Asset Libraries Activating proper Data Layers (including non-Deco Layers like Weather, etc.) Validate Decoration placements I also made it possible to subclass the DecoActor to customize behavior for different Level types. For example, the default Decoration logic did not satisfy outdoor Levels the way it did indoor Levels. Designers were able to configure what schema was followed during runtime. I also supported Editor-only testing by exposing DecoActor functionality to the editor, allowing designers to test the Decorations without needing to run the game.\nDecoVolume\nThis system heavily utilized Procedural Content Generation (PCG) to decorate the space once all navigation-blocking objects have been placed. I created custom DecoVolumes for designers and marked objects to spawn, and are the area in which PCG utilizes to decorate. These Volumes get sampled and their relevant PCG Graphs complete the final layer of decoration.\nDesigners can specify on the Volume which PCG Graphs will be used to decorate the space within. These Volumes respect existing Decorations, player-placed objects, and cameras. Multiple graphs can apply to each Volume, and a hierarchy of importance between graphs is established in order of appearance in the Property to determine which graph\u0026rsquo;s objects are prioritized.\nDecoLibrary\nWe utilized hierarchial, random containers built in-house similar to Unreal Choosers to store the Asset Libraries. These containers held entries that were able to be populated with the Assets and tag them with necessary metadata. Sockets queried these Libraries depending on what type of Asset they were expecting. These Libraries rejected Assets that fell outside Transform and Orientation requirements and also handled random choosing behavior.\nSystem Architecture\r#\rThere are two main processes that the Decoration system is responsible for: Decorating and Redecorating. The first time a Level is decorated is the most important, because Decorations persist as the Level gets reused in the same narrative, just updated.\nAssets are selected from the queried Libraries based on both runtime narrative context and the metadata provided by the populated Sockets. The Decoration system collects this context and passes the Socket and Asset descriptions through a proprietary embeddings subsystem, which produces similarity scores based on natural-language semantics. These scores are evaluated alongside structured metadata constraints to determine the best match for each Socket. The highest-scoring Assets are then utilized for the Sockets.\nAnother concept that I utilized heavily in this system is the idea of Pinned objects. We did not want the system to entirely redecorate a Level every time a player revisted it. However, as to real life, it also would not make sense for a space to stay identical after the campaign and time have continued to progress. The Decoration system is responsible for systematically Pinning objects that are determined as characteristic to the scene, and these Pinned objects will persist with the Set until the campaign has finished. Pinned objects often included:\nNavigation-Blocking Furniture Narrative-important Assets Characteristic decorative pieces Player-placed Assets Reflection\r#\rThis system was both technically involved and highly visible to players, which made correctness, flexibility, and iteration speed equally important. Building it shaped how I approach large-scale, player facing systems, and taught me some of these key learnings:\nPolicy-based design decoupled configurations and schemas from core decoration logic, allowing the system to remain modular and free of dependencies. Early Asset validation at the library level prevented subtle runtime issues and shifted focus from many small, reactive fixes and towards a more proactive, intentional approach to data authoring. Implementing Sockets using component-based architecture allowed us to build Levels with layered policies, creating more complex and varied results while still respecting gameplay requirements and narrative accuracy. Tag-driven Pipelines streamlined Asset management and filtering, allowing designers to mark up assets as needed while integrating cleanly with the code backend without limiting variation. ","externalUrl":null,"permalink":"/games/burbank/set-deco/","section":"Games","summary":"","title":"Procedural Interior Decoration","type":"games"},{"content":"\rOverview\r#\rA main goal that I had as Creative Director was to deliver an experience that was both fun to play but also visually appealing and whimsical, which immersed the player in the magical world Interweaver took place in. The Art Team and I settled on a stylized, almost hand-drawn aesthetic early into production, which was ideal for our smaller team of emerging artists.\nI personally played a key role in this initiative as the main Technical Artist, and was responsible for the majority of shaders, VFX, and post-production effects that shipped with the final product and helped define the game\u0026rsquo;s visual identity. I was also responsible for ensuring they were seamlessly integrated into the game\u0026rsquo;s systems and animations without causing any performance issues, including lag or frame drops.\nShaders/HLSL\r#\rI was responsible for authoring the vast majority of shaders used throughout Interweaver, which were applied to objects, the game environment, and supplemented visual effects. These shaders were crucial in defining the game\u0026rsquo;s distinct visual aesthetic. I created most of my shadersk using ShaderGraph in Unity, with a few coupled with HLSL for more advanced effects and to optimize performance when necessary.\nWater Shader\r#\rOne of the most challenging shaders I created for this project was our Water Shader. This involved a ShaderGraph with multiple Subgraphs and was not only challenging to conceptualize, but to translate into a functional shader that met both aesthetic and performance requirements.\n[water shader photo here]\nDevelopment Process\nFirst Iteration Second Iteration This shader involved many different components, including wave distortion, depth fade, reflections, refraction, ripple texture, and foam. It was also performant due to favoring calculated effects over expensive texture sampling, and utilized shared ShaderGraph subgraphs to avoid unnecessary shader variants. This allowed the shader to scale across many scenes and water volumees without increasing draw calls or shader complexity.\nThe shader worked by using the camera plane to project scene color onto the water surface, enabling dynamic reflection and distortion effects. While the effect looked great from a static camera angle, it introduced a challenge at times when one water volume could be viewed from varying heights or angles. Each piece was created with multiple exposed variables to tweak per instance so it was highly reusable.\nRipple Shader\r#\rTo compliment the Water Shader and to ensure the player\u0026rsquo;s movement felt natural as they moved through the water, I created an accompanying Ripple shader that dynamically trailed the player through water.\n[make into final ripple shader video] Unlike the Water Shader, which was built in ShaderGraph with various subgraphs, the Ripple shader was implemented as a custom .shader written in HLSL and driven through C#. It operated using RenderTextures that updated each frame with the player\u0026rsquo;s position to simulate ripple propogation. The existing waves from the Water Shader presented a technical challenge for this effect, so I implemented this shader to sample neighboring pixels from the current and previous frames to simulate natural wave propogation without introducting additional vertex displacement, as the surface motion was being handled by the primary Water shader.\nI included a bit of the implementation below:\nIEnumerator ripples() { AddMat.SetTexture(\u0026#34;_ObjectsRenderTexture\u0026#34;, ObjectsRenderTexture); AddMat.SetTexture(\u0026#34;_CurrentRenderTexture\u0026#34;, CurrRenderTexture); Graphics.Blit(null, TempRenderTexture, AddMat); RenderTexture renderTexture = TempRenderTexture; TempRenderTexture = CurrRenderTexture; CurrRenderTexture = renderTexture; // Calculate the ripple animation using ripple shader. RippleMat.SetTexture(\u0026#34;_PrevRenderTexture\u0026#34;, PrevRenderTexture); RippleMat.SetTexture(\u0026#34;_CurrentRenderTexture\u0026#34;, CurrRenderTexture); Graphics.Blit(null, TempRenderTexture, RippleMat); Graphics.Blit(TempRenderTexture, PrevRenderTexture); // Calculate the result for the next frame. RenderTexture rt = PrevRenderTexture; PrevRenderTexture = CurrRenderTexture; CurrRenderTexture = rt; yield return null; StartCoroutine(ripples()); }\rOverall Game Shader\r#\rTo achieve the intended hand-painted, stylized aesthetic, I created a csutom shader that defined the visual style of the game. It uses techniques from cel and subsurface scattering shaders to create a the soft, painted vibe I wanted. It was important that the shader simplified lighting, controlled shadow thresholds, and preserved color vibrancy to support our stylized, hand-painted aesthetic.\n[shader before and after]\nCloud Shader\r#\rOne of our levels was took place partially high up in the clouds on floating islands. To give the effect of being high in the sky and control player visibility across certain areas of the level, we incorporated volumetric clouds in this section. This created the need for a custom shader because we needed a way to support player movement through the clouds while also preserving their volumetric depth, wispiness, and transparency.\nTo solve this, I implemented a raymarching technique to render volumetric clouds in 3D space that maintained depth and transparency to allow the camera to pass physically through them and maintain visual fidelity. You can see a snippet below explaining how the shader raymarches through a 3D density field while performing secondary light sampling to approximate scattering and shadowing inside the cloud volume.\nfor (int i = 0; i \u0026lt; numSteps; i++) { rayOrigin += (rayDirection * stepSize); // The blue dot position float3 samplePos = rayOrigin + offset; float sampledDensity = SAMPLE_TEXTURE3D(volumeTex, volumeSampler, samplePos).r; density += sampledDensity * densityScale; float3 lightRayOrigin = samplePos; for (int j = 0; j \u0026lt; numLightSteps; j++) { // The red dot position lightRayOrigin += -lightDir * lightStepSize; float lightDensity = SAMPLE_TEXTURE3D(volumeTex, volumeSampler, lightRayOrigin).r; // The accumulated density from samplePos to the light - the higher this value the less light reaches samplePos lightAccumulation += lightDensity; } // The amount of light received along the ray from param rayOrigin in the direction rayDirection float lightTransmission = exp(-lightAccumulation); float shadow = darknessThreshold + lightTransmission * (1.0 - darknessThreshold); // The final light value is accumulated based on the current density, transmittance value and the calculated shadow value finalLight += density * transmittance * shadow; // Its value is updated at each step by lightAbsorb, this sets the light lost by scattering transmittance *= exp(-density * lightAbsorb); } transmission = exp(-density); result = float3(finalLight, transmission, transmittance);\rVFX Showcase\r#\rAlong with the Shaders, I created 30+ VFX and particle systems for Interweaver to supporting the game\u0026rsquo;s visual direction and providing players with meaningful feedback as they used mechanics and progressed through the game. These were mainly done in Unity\u0026rsquo;s VFX Graph and involved custom 3D models, textures, and calculations for particle behavior.\nHere I will showcase most of the effects I made for Interweaver below. I tried to include the software used to create the effects as well as any additional interesting information:\n(all of these are temp until i can grab better images tehe :)\n[stag teleport effect]\n[clouds]\n[etc]\n","externalUrl":null,"permalink":"/games/interweaver/shaders-vfx/","section":"Games","summary":"","title":"Shaders \u0026 VFX","type":"games"},{"content":"\rOverview\r#\rCharacter relationships are at the core of Burbank - they drive the player to make gameplay decisions, explore parasocially, and are a vessel to experience some of Burbank\u0026rsquo;s most emotional moments. I was responsible for creating the base Relationship system that manages all 2-character relationships and the gameplay impact of the player character\u0026rsquo;s choices in-game. All characters started at a base \u0026lsquo;Acquaintance\u0026rsquo; Relationship, but were able to progress through a Map of possible relationships with any given NPC. Each Relationship affected the gameplay differently, and had different conditions to achieve.\nRelationship Design\r#\rAs we were designing how Relationships would manifest in the game, we closely examined how people communicate in real life to pinpoint the most exciting moments to play out through our game. This led us to our current design, which involves the player character navigating through a \u0026lsquo;Map\u0026rsquo; of different Relationship stages. All characters start at a default Acquaintance relationship, and depending on how the player interacts with other characters and the world around them, their relationships with individual characters can progress to different pre-prescribed Relationship Stages.\nA small snippet of the Relationship Map in which the player\u0026rsquo;s character could navigate: For the player to change their character\u0026rsquo;s Relationship with any given character in the Level, they must meet specific criteria tied to the next Stage. Each Stage requires the Player to complete certain actions or gameplay decisions in order to move forward in their relationship. For example, if the Player successfully pulls off a riskier romantic advancement on another character, their Relationship could progress straight to Dating, rather than Flirting. This created more dynamic gameplay not just between playthroughs, but also created complex inter-character relationships in game that drew the Player themselves in.\nRelationship Stages were also associated with their own Gameplay Modifiers, Effects, and more. If the Player tried flirting with a character while actively in a Romantic relationship, other NPCs would react accordingly (for better or worse) and the Player receives gameplay buffs and debuffs depending on their runtime choices.\nSystem Architecture\r#\rRelationships systematically have 2 major components to them: a Relationship Data Asset and a Relationship State Actor. The team created the Data Assets during production, and they contributed to the systematic representation of the Relationship Map. Each Relationship pair has an owning State Actor, which is responsible for the involved characters and is the runtime representation of a Relationship. These State Actors keep track of which Relationship the two characters have at any given time and maintain a reference to the relevant Relationship Data Asset. The Properties defined by this Asset dictates this Relationship\u0026rsquo;s effect on the game.\nRelationship Data Asset\r#\rWe utilized custom Data Assets heavily throughout Burbank to store game states and the Relationship system is no different. The custom Relationship Asset I made is the systematic representation of a Relationship Stage, such as \u0026lsquo;Best Friend\u0026rsquo;, \u0026lsquo;Married\u0026rsquo;, \u0026lsquo;Rival\u0026rsquo;, etc.\nSome of this Asset\u0026rsquo;s Properties include:\nContext to be read during runtime for the involved characters Relationships that can be progressed to from this Relationship The criteria and gameplay requirements to progress this Relationship Effects and gameplay modifiers that occur when the Player character has this Relationship active The Relationship State Actor holds a reference to one Relationship Data Asset at a time. It reads the Asset\u0026rsquo;s Properties at runtime with gameplay context to determine how to progress throughout the Relationship Map.\nRelationship State Actor\r#\rRelationship State Actors are responsible for the relationship between 2 characters at runtime, and are created by the system automatically for all existing characters at a base \u0026lsquo;Acquaintance\u0026rsquo; relationship to start the game. They listen for game state events and delegates to trigger Relationship events as well as applying relevant modifiers and Game Effects to the campaign.\nThese State Actors were responsible for managing a variety of data at runtime efficiently and visually elegantly, such as:\nhandling asynchronous processes to ensure non-blocking gameplay tracking and managing a variety of game systems using modular design principles, ensuring flexibility as we continued to update system designs subscribed to the Event Bus system used throughout the game to listen for game events to facilitate clean communication across systems and reducing direct dependencies between systems integrating various game state delegates to handle gameplay interactions maintaining an Inventory system to cleanly track and manage the Player\u0026rsquo;s Relationship progression In-Game Representation\r#\rAs Players developed their characters\u0026rsquo; Relationships, they were able to unlock narrative and cinematic moments with the other participant. While the exact Relationship Map is not player-facing, a lot of progression criteria and other relationship state data is able to be viewed in the UI. Any information not immediately clear to the player could be deduced from interactions with characters that the Player character has relationships with passively via gameplay. Environment interactions and gameplay reactions to the player\u0026rsquo;s choices implicitly guided them towards narrative-rich interactions.\nMuch like how strengthening relationships in real life leads to more meaningful moments and impacts your day to day more, building stronger bonds with other characters unlock richer gameplay opportunities. Burbank\u0026rsquo;s characters had a dynamic memory system, allowing them to recall significant moments from the player\u0026rsquo;s campaign to help shape their relationships based on both the player\u0026rsquo;s intentional and passive choices throughout the world.\nReflection\r#\rThis system scaled across narrative, game state, and player choices, making it central and impactful to the game\u0026rsquo;s overall experience. Designing it made me analyze turning organic real-world experiences into structured, state-driven systems, and taught me several key lessons about building systemic, player-facing gameplay architecture:\nState-driven design formalized each Relationship stage as a Data Asset and separated authored intent from runtime execution. This allowed designers to iterate on the Relationship Map without rewriting code or heavily editing dependent Assets. By subscribing to the global Event Bus, scene events, and campaign Quests, the system avoided directly querying other systems and ensured reliability as new gameplay and narrative features were added. Representing progress as inventory allowed in-game events and choices to naturally contribute toward relationship advancement rather than requiring custom logic for every possible interaction. Transition flexibility allowed relationship changes to be triggered through different types of gameplay while still running through the same core system, preserving both cinematic pacing and technical reliability. ","externalUrl":null,"permalink":"/games/burbank/relationships/","section":"Games","summary":"","title":"Character Relationship System","type":"games"},{"content":"\rOverview\r#\rInterweaver is an isometric puzzle-platformer that features systems designed to extend beyond individual puzzles or levels, making it so that we, as designers, could gradually increase the complexity of puzzles while also allowing players to retain and build upon learned skills as they progress. I was responsible for designing and authoring a few of the more foundational systems that other mechanics and puzzles were built upon. I did this modularly to reduce ongoing engineering maintenance as designers iterated on levels, making it easy for new mechanics to be added without reworking existing systems.\nBelow are a few examples of foundational game mechanics that I wrote that served to accentuate our puzzles and our core player abilities:\nSensor System\r#\rTo avoid overcomplicating puzzles with excessive mechanics, we relied on reusable systems that could be layered and combined in ways to increase difficulty, complexity, and create more fun puzzles. One of the biggest components of this mindset is the Sensor System, a system that relied on pressure plates and other sensors that triggered or activated different puzzle elements.\nIt is used heavily in all levels, mainly for driving puzzle logic and platforming progression. It solved the need for one-off mechanics with bespoke logic needing maintenance, and allowed us to build a wide range of puzzles using a single, consistent system.\n[make into image of in game] This system was built around a centralized, event-driven Controller that decoupled the sensor state from individual object behavior. This allowed any gameplay elemeny to subscribe to sensor state changes without bespoke logic, which allowed designers to create complex puzzles using these systems as a basis to avoid backend dependency issues as we iterated. This separation kept puzzle logic readable, reduced maintenance overhead, and supported scalable puzzle design across the entire game.\nSome examples of gameplay elements that were triggered with sensors include:\nDoors Moving or appearing platforms Object spawners Through building this system, I implemented several key architectural patterns, including:\nEvent-Driven Logic\nSensors expose UnityEvents rather than directly referencing the GameObjects they affect. Designing the sensors as an event-driven system decoupled activation detection from puzzle logic, supporting creative puzzle design and solutions while maintaining backend stability and preventing softlocks.\nMulti-Sensor Compatibility\nActivated GameObjects can be powered or activated with multiple Sensors as part of the core system design, per designer setup. This further enabled them to scale puzzle complexity without requiring new logic.\nDeterministic State Handling\nDeterministic state handling is acheived through explicit state flags, guarded Coroutine execution, and sensor state validation, ensuring objects and sensors respond correctly both logically and visually when sensor state changes mid-interaction.\nLight Energy System\r#\rTo compliment the aforementioned Sensor system, Light became an important concept in the later Levels and was our way of building upon the more intuitive sensors. Light was a way to power other sensors and trigger energy beams, and occasionally even protected the player from hazards. The Light system heavily influenced how we built our later puzzles and platforming challenges as we were able to explore this venue of difficulty through our more intuitive/simple/average mechanic of sensors.\nThis system also includes crystals that could refract or redirect light beams when activated, creating chained interactions that powered other crystals and created layered puzzles. When a crystal became active, its beam activates nearby crystals or sensors by updating centralized light state and triggering activation logic.\n[get in-game screenshot of beams to put here] The system worked as a state-driven extension of the Sensor system, and worked in tandem with the sensors and other light crystals to create unique, collision-based puzzle interactions. Light crystals and beams communicated through a centralized state and via activation flags, which allowed light to power multiple systems, including itself.\nActivated crystals emitted focused light beams that detected and powered other crystals and sensors through ray-based collision checks and their shared light state. This allowed light to chain and bounce across multiple objects at runtime, forming dynamic power paths without hard-coded connections. This made light a flexible extension of the Sensor System, supporting more complex puzzles while remaining deterministic and easy to reason about.\nThis system supported our design policy of scaling puzzle complexity without increasing engineer workloads, using a small set of flexible mechanics that could be composed in a wide variety of ways. I personally enjoyed designing and coding this mechanic because of its endless potential in puzzle or other gameplay systems, and I wish we had the scope for all of the ideas we had for this mechanic come to life.\n","externalUrl":null,"permalink":"/games/interweaver/puzzles/","section":"Games","summary":"","title":"Puzzle Systems","type":"games"},{"content":"\rOverview\r#\rInterweaver began as a concept I developed about the world, the main characters, and their abilities in-game. Out of around 40 game pitches, it was selected as one of three concepts to be developed into a fully-realized game by popular vote. As we progressed through development, Interweaver\u0026rsquo;s many distinctive features evolved to make the game into what it is today.\nThe team consisted of 14 students, and together with my Producer we established a leadership team and established production pipelines for art, code, and design. We fostered a collaborative environment that encouraged creative discussion from the team whenever possible, and I maintained ownership of the creative direction, approving all production-ready assets to ensure everything shipped with our quality and visual standards.\nConcept Art + Brainstorms\r#\rI wanted to illustrate some of the early concepts that I had for Interweaver to give some context on how the game evolved to what you see on Steam today. It is important to call out that this project was developed on a 35-week timeline with Senior Design students of various skill levels, and throughout development, we encountered a range of technical, leadership, and design challenges that influenced how the original concept evolved into the final game. I learned so much about the full game development process and what it takes to lead effectively in a fast-faced production environment. Enjoy :)\nCharacters\r#\rThe Weaver\nEarly Concept Character Look-Dev Weave Ability Initial Character Model Rendered Snapshot The main antagonist, The Rival\nEarly Concept Character Look-Dev Initial Character Model The Familiars\nEarly Concepts Final Concept Art (I couldn\u0026rsquo;t find the owl ;-;)\nRendered Snapshots [mole snapshot]\nThe Bosses\nEarly Concepts Final Concept Art Rendered Snapshots\n[hawk snapshot]\n[dragon snapshot]\n","externalUrl":null,"permalink":"/games/interweaver/concept/","section":"Games","summary":"","title":"Creative Vision \u0026 Concepts","type":"games"},{"content":"\rOverview\r#\rDue to our small team size and rapid iteration time, it was imperative we spent the least amount of time creating and organizing Assets in mass as we were building our Levels. To support this, I integrated creating gameplay systems with Slate tools and editors as part of my regular workflow to expedite the process of asset creation and setup. This was exemplified throughout the project but I will highlight bigger editors and tools below.\nRelationship Editor\r#\rOne of the most utilized editors I created for Project Burbank is the Relationship Editor and related tools. The need for this editor arose when we were building out the Relationship Map, and it became hard to keep track of how the various Relationships were able to progress. We needed a way for our designers to visualize how each Stage of the Relationship Map was able to evolve and easily compare the effects and constraints of them all.\nThis custom Asset Editor shows a graph visualizing the Relationship Stages that can be progressed to from the Relationship Asset being viewed currently. I based it off of the Reference Viewer built into UE, and utilized custom EdGraph and EdGraphNode classes to build the progression Map. The editor also includes custom tools to walk designers through setting up complex Properties and a Context Panel that appears to help focus the information shown at once.\nRelationship Graph\r#\rI created the Graph to replace the experience of editing a static Details panel, and since each field on the Relationship Data Assets required heavy setup and often depended on other Properties or Assets, there were many places to implement custom flows to create a seamless experience as we built the Map. Interacting with the Graph would implicitly prompt designers to edit corresponding settings on the Relationship, and the Graph highlights relevant nodes, Properties, or errors in response. This editor helped us visualize and rapidly test different variations of Relationships and progression settings by making it simple to edit. The modular design of the Relationship system also supported this notion, and allowed us to create complex and engaging moments for our Players via these Relationships.\nSystem Design\nI utilized a View Model-driven approach, where the View Model serves as the central hub for all systematic logic connecting the Slate widget with game code and runtime data. The View Model acts as a mediator to ensure that the editor\u0026rsquo;s UI remains in sync with the game state while also managing communication with broader game systems and event handling.\nWith this setup:\nSRelationshipEditor::Construct(const FArguments\u0026amp; InArgs, TSharedPtr\u0026lt;FRelationshipEditorViewModel\u0026gt; InViewModel) { GraphObj = NewObject\u0026lt;UEdGraph_RelationshipEditor\u0026gt;(); GraphObj-\u0026gt;AddToRoot(); RelationshipEditorViewModel = InViewModel; ... GraphObj-\u0026gt;ConstructGraph(InViewModel); // Create the Graph Editor GraphEditorPtr = SNew(SGraphEditor) .GraphToEdit(GraphObj); ... // Add all other widgets } UEdGraph_RelationshipEditor::ConstructGraph() { // Use ViewModel information to determine which node is focused/should be centered UEdGraphNode_Relationship* CenterNode = SpawnNode(ViewModel, FVector2D(0.0f, 0.0f)); ... // Create graph ... ViewModel-\u0026gt;GraphEditor = this; }\rIt was then possible to retrieve the View Model from any relevant editor context to trigger or access data, like so:\nSRelationshipNode::OnMouseButtonDown(const FGeometry\u0026amp; Geometry, const FPointerEvent\u0026amp; MouseEvent) { UEdGraph_RelationshipEditor* Graph = GetGraph(); // If Graph and all relevant data is valid Graph-\u0026gt;GetEditorViewModel()-\u0026gt;HandleNodeSelected(GetNode()); }\rUsing a ViewModel to manage the connection between the editor\u0026rsquo;s Slate UI and game data cleanly decouples the UI from game logic. This modularity makes it easy for us to continue to iterate on the underlying Relationship system without worrying about maintaining the editor, since new features can be added without disrupting any other functionality.\nRelationship Editor Tools\r#\rI wanted it to be as easy as possible for designers to interact with and edit Relationships via the graph, so I also included tools within the editor to walk designers through setting up more complex Properties, like adding new Stages to progress to. Because this system was so complex as well, it was easy to create duplicates, forget to assign data to Properties, or make various other mistakes. This tool was also responsible for throwing errors should any information be inputted wrong to further expedite updating the Map.\nThis New Transition overlay was implemented as a standalone Tool that appeared as a popup when designers used a Button in the editor header linked to a custom UI command. This separate flow allowed designers to author each new progression stage as an explicit, state-driven process rather than ad hoc Property edits. This particular Tool consists of multiple steps that owns a narrowly scoped slice of data, so designers can set up these Transitions without leaving the workflow to hunt down related assets, contexts, or other dependencies elsewhere in the editor. Separating the process into sequential steps ensured that missing or conflicting data is identified and resolved at the point of entry, before it propogates into downstream systems and affecting other parts of the system - something that was incredibly important in such a fast-paced iteration environment.\nEmbedded Text Editor\r#\rAnother complication I had to solve working on this project was managing the amount of asynchronous processes that occured both at runtime and, more importantly here, during editor-time. These non-latent functions occasionally allowed designers to close the editor while processes were still running, which led to crashes or unsaved changes. I created a custom Property for all Assets that used the problematic interface that informed designers when their data was still processing, overrode the default Unreal Asset save behavior, and included a button to manually trigger the asynchronous process.\nTo prevent Assets from being saved in an incomplete state, I intercepted and modified the default save behavior on objects implementing the Embeddings interface so the Asset remained unsaved until all data has finished processing. When designers trigger a save, the asset is marked as dirty as non-latent processes are started and registered with an editor-only subsystem that tracks pending embedded Assets. Once all data is returned and proper delegates have been broadcasted, the Asset is marked clean, cleaned from the pending queue, and saved by the system. This prevents partial data from being serialized and written to disk and reduced the number of crashes caused during this workflow.\n","externalUrl":null,"permalink":"/games/burbank/slate-ui/","section":"Games","summary":"","title":"Slate Tooling","type":"games"},{"content":"Below are games that I have worked on professionally, academically, and personally. These are games in which I have created and owned systems that shipped with the final product and were instrumental in both production and deployment. All content featured is original work that I own and am authorized to share.\nSee below for showcases, technical design documents, system designs, and more.\n","date":"1 May 2025","externalUrl":null,"permalink":"/games/","section":"Games","summary":"","title":"Games","type":"games"},{"content":"","date":"1 May 2025","externalUrl":null,"permalink":"/","section":"Peyton Bischof","summary":"","title":"Peyton Bischof","type":"page"},{"content":"\rGameplay Engineer\r#\rProject Burbank is an unreleased life sim developed by a small team of industry veterans in Unreal Engine 5 at Midsummer Studios in Hunt Valley, MD. Burbank uniquely explores player connection with characters and storylines through life sim/visual novel sandbox-style gameplay. I am responsible for several core systems and have a hand in nearly every feature of the game.\nThis project was unique in its timeframe - we rapidly designed, engineered, and iterated on all systems and pipelines to meet tight demo deadlines. As a result, as well as creating gameplay mechanics in C++ and Blueprint, I also became fluent in Slate to customize our Editor and tooling to support our designers as we rapidly iterated on the games\u0026rsquo; development.\nDevelopment Log\r#\rDue to the small team size of 13, my skills were leveraged in a variety of ways to support all aspects of the game. Below are some of the major systems and features I contributed to Project Burbank:\n","date":"1 May 2025","externalUrl":null,"permalink":"/games/burbank/","section":"Games","summary":"","title":"Project Burbank","type":"games"},{"content":"\rCreative Director, Gameplay Engineer, Lead Technical Artist\r#\rInterweaver is an isometric puzzle-platformer built in Unity by a small team of students from the University of Central Florida as our Senior Design Capstone. I served as the Creative Director and was also responsible for programming a variety of major systems and mechanics in C# and created the majority of the visual effects in Unity\u0026rsquo;s Shader and VFX Graph.\nThis was my first experience leading a full development team of designers, artists, and programmers. I was at the forefront of production and development, and formed a leadership team that quickly became comfortable with Agile development strategies to create the best player experience during our 35-week production period.\nYou can find the Steam page here.\nDevelopment Log\r#\rI contributed heavily to this project from all aspects of production: code, art, and design. Below I will go into depth on major systems, features, and effects I am responsible for, as well as trying to explain some design decisions and philosophies that I learned during this experience.\n","date":"20 May 2024","externalUrl":null,"permalink":"/games/interweaver/","section":"Games","summary":"","title":"Interweaver","type":"games"},{"content":"","externalUrl":null,"permalink":"/contact/","section":"Peyton Bischof","summary":"","title":"About","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"\r","externalUrl":null,"permalink":"/resume/","section":"Peyton Bischof","summary":"","title":"Resume","type":"page"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]